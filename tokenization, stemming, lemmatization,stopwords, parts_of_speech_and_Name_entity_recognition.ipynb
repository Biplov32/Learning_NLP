{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R9P-n1u5SJaa",
        "outputId": "b0d74f9b-20b1-48aa-93c4-e722c9bbdad1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.11/dist-packages (3.9.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk) (8.1.8)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.11/dist-packages (from nltk) (2024.11.6)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from nltk) (4.67.1)\n"
          ]
        }
      ],
      "source": [
        "!pip  install nltk"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk"
      ],
      "metadata": {
        "id": "ClRzz6pXTcRC"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "corpus = \"\"\"Hello wlcome, to biplov's tutorial on Natural Language Processing.\n",
        "Please do watch the entire notebook! to become a expert in NLP\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "-LysPesHSTwc"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(corpus)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MXey0DlIS4DS",
        "outputId": "f4b6532f-570e-4120-cda6-c78a33d8dbae"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hello wlcome, to biplov's tutorial on Natural Language Processing.\n",
            "Please do watch the entire notebook! to become a expert in NLP\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('punkt_tab')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KmvJBFa5TTLO",
        "outputId": "4a4123dd-4389-4e7b-9127-980481638961"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Tokenization\n",
        "#Sentence to Paragraph\n",
        "from nltk.tokenize import sent_tokenize\n",
        "documents = sent_tokenize(corpus)"
      ],
      "metadata": {
        "id": "YCTvgQGRS7Zy"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for sentence in documents:\n",
        "  print(sentence)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MgUi9mxnTo31",
        "outputId": "06003b01-213b-4281-fa15-3961f7ef7c72"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hello wlcome, to biplov's tutorial on Natural Language Processing.\n",
            "Please do watch the entire notebook!\n",
            "to become a expert in NLP\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "##Tokenization\n",
        "##Paragraph into words\n",
        "##Sentence into words\n",
        "from nltk.tokenize import word_tokenize"
      ],
      "metadata": {
        "id": "rpHq3l67UPSg"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "word_tokenize(corpus)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Yg41NB2bUhlv",
        "outputId": "2b3922ef-22fc-4754-c1b4-efd0d727639d"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Hello',\n",
              " 'wlcome',\n",
              " ',',\n",
              " 'to',\n",
              " 'biplov',\n",
              " \"'s\",\n",
              " 'tutorial',\n",
              " 'on',\n",
              " 'Natural',\n",
              " 'Language',\n",
              " 'Processing',\n",
              " '.',\n",
              " 'Please',\n",
              " 'do',\n",
              " 'watch',\n",
              " 'the',\n",
              " 'entire',\n",
              " 'notebook',\n",
              " '!',\n",
              " 'to',\n",
              " 'become',\n",
              " 'a',\n",
              " 'expert',\n",
              " 'in',\n",
              " 'NLP']"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for sentence in documents:\n",
        "  word_tokenize(sentence)"
      ],
      "metadata": {
        "id": "KhYWp057UlZP"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import wordpunct_tokenize\n",
        "wordpunct_tokenize(corpus)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VWMHUBD9VE1z",
        "outputId": "a84ddfd8-2c06-48c7-f8a7-a7eff273c233"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Hello',\n",
              " 'wlcome',\n",
              " ',',\n",
              " 'to',\n",
              " 'biplov',\n",
              " \"'\",\n",
              " 's',\n",
              " 'tutorial',\n",
              " 'on',\n",
              " 'Natural',\n",
              " 'Language',\n",
              " 'Processing',\n",
              " '.',\n",
              " 'Please',\n",
              " 'do',\n",
              " 'watch',\n",
              " 'the',\n",
              " 'entire',\n",
              " 'notebook',\n",
              " '!',\n",
              " 'to',\n",
              " 'become',\n",
              " 'a',\n",
              " 'expert',\n",
              " 'in',\n",
              " 'NLP']"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import TreebankWordTokenizer\n",
        "#Fullstop will not be treated as seperate word , only treated as seperate word for last fullstop"
      ],
      "metadata": {
        "id": "dTSYLHVvVQuS"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = TreebankWordTokenizer()"
      ],
      "metadata": {
        "id": "awqUtmUKVeFC"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer.tokenize(corpus)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h8mC-_jDVoEe",
        "outputId": "c0c0a602-8538-4f5c-cc7a-a74b620faac6"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Hello',\n",
              " 'wlcome',\n",
              " ',',\n",
              " 'to',\n",
              " 'biplov',\n",
              " \"'s\",\n",
              " 'tutorial',\n",
              " 'on',\n",
              " 'Natural',\n",
              " 'Language',\n",
              " 'Processing.',\n",
              " 'Please',\n",
              " 'do',\n",
              " 'watch',\n",
              " 'the',\n",
              " 'entire',\n",
              " 'notebook',\n",
              " '!',\n",
              " 'to',\n",
              " 'become',\n",
              " 'a',\n",
              " 'expert',\n",
              " 'in',\n",
              " 'NLP']"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Stemming and It's types- Text Preprocessing"
      ],
      "metadata": {
        "id": "dGBDqeGkVvum"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "##Classification Problem\n",
        "##Comments of product is a positive review or negative review"
      ],
      "metadata": {
        "id": "Ac4M4O9aWheA"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "words = [\"eating\", \"eats\", \"ate\", \"adjustable\", \"rafting\", \"ability\", \"meeting\"]"
      ],
      "metadata": {
        "id": "Qu-sCzMOWrnb"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## PorterStemmer"
      ],
      "metadata": {
        "id": "ExbZZwYRalOE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import PorterStemmer\n",
        "stemmer = PorterStemmer()"
      ],
      "metadata": {
        "id": "AgiZgvOWa9j9"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for word in words:\n",
        "  print(word, \":\", stemmer.stem(word))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PM8odkiObCEK",
        "outputId": "2d9fa788-8ee7-4977-defa-b5fb1f651771"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "eating : eat\n",
            "eats : eat\n",
            "ate : ate\n",
            "adjustable : adjust\n",
            "rafting : raft\n",
            "ability : abil\n",
            "meeting : meet\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "stemmer.stem(\"congratulation\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "nuJNsXiTbLur",
        "outputId": "12d85178-dd3c-468e-b2c8-61cd2b69c45f"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'congratul'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## RegexpStemmer class"
      ],
      "metadata": {
        "id": "yCAmVqZjbkNQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import RegexpStemmer\n",
        "reg_stemmer = RegexpStemmer('ing$|ed$')"
      ],
      "metadata": {
        "id": "Wgmtc3B_cB8l"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "reg_stemmer.stem('eating')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "CR4gT3oUcHMT",
        "outputId": "6656018f-e59c-4843-ae36-3720e763e347"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'eat'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Snowball Stemmer"
      ],
      "metadata": {
        "id": "ny049A0ocv3Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import SnowballStemmer"
      ],
      "metadata": {
        "id": "2E6c_hL3dHSc"
      },
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "snowball_stemmer = SnowballStemmer('english')"
      ],
      "metadata": {
        "id": "73IGdS3xdav1"
      },
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for word in words:\n",
        "  print(word, \":\",snowball_stemmer.stem(word))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vdHcsep0dfM1",
        "outputId": "3f473292-3d45-4988-e5ec-2dcba6270058"
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "eating : eat\n",
            "eats : eat\n",
            "ate : ate\n",
            "adjustable : adjust\n",
            "rafting : raft\n",
            "ability : abil\n",
            "meeting : meet\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Lemmatization"
      ],
      "metadata": {
        "id": "v8bv14ujd8Dy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import WordNetLemmatizer"
      ],
      "metadata": {
        "id": "ehjkH6tzejH3"
      },
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('wordnet')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zYQ2AYP7fnjX",
        "outputId": "622f2153-60d0-49c0-bae5-7b9af15b3475"
      },
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 53
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "lemmatizer = WordNetLemmatizer()"
      ],
      "metadata": {
        "id": "25cnunDsfQ3c"
      },
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for word in words:\n",
        "  print(word, \":\", lemmatizer.lemmatize(word, pos ='v'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hbohjwXSfaK_",
        "outputId": "99cc2c5b-0f1c-461f-9557-ab1e2fd8e2c8"
      },
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "eating : eat\n",
            "eats : eat\n",
            "ate : eat\n",
            "adjustable : adjustable\n",
            "rafting : raft\n",
            "ability : ability\n",
            "meeting : meet\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Stopwords"
      ],
      "metadata": {
        "id": "nZ1BtF4Xfdan"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "paragraph = \"\"\"\n",
        "  I grew up in the Eastern Cape,\n",
        "  an area of South Africa much like the one so well described to you by Thomas Hardy.\n",
        "  It was a world of oral tradition, healing properties were herbal, an abscess would be treated with poultices, clean water was simply not available.\n",
        "  Gastro intestinal infections, malaria, cholera were rampant.\n",
        "  Life was brutish and short.\n",
        "  Electricity and the horseless carriage did not exist for me.\n",
        "  The hardness and poverty of existence was aggravated by an uncaring society.\n",
        "  Colonies were meant to be exploited both for the mother country and for those who came to settle in our area.\n",
        "  Fresh in the memories of the older generation of the poorest of the very poor, and spoken of in low hushed tones, what was said to be a hugely profitable business that was said to have been abolished – that was the business of slavery.\n",
        "  My youth and young adulthood was spent with others in fighting an unjust and oppressive system.\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "LcxdmNpBhs1h"
      },
      "execution_count": 67,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import PorterStemmer"
      ],
      "metadata": {
        "id": "ytp2C1sLiraS"
      },
      "execution_count": 70,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.corpus import stopwords\n",
        "nltk.download('stopwords')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eqUBlBjqjT11",
        "outputId": "38099ef6-c5e5-4ada-df2c-4f8b63220866"
      },
      "execution_count": 72,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 72
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "stopwords.words('english')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uydMsW0IjZ4Y",
        "outputId": "30e82157-b0fc-4cda-a7ca-a2144aaccf7a"
      },
      "execution_count": 73,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['i',\n",
              " 'me',\n",
              " 'my',\n",
              " 'myself',\n",
              " 'we',\n",
              " 'our',\n",
              " 'ours',\n",
              " 'ourselves',\n",
              " 'you',\n",
              " \"you're\",\n",
              " \"you've\",\n",
              " \"you'll\",\n",
              " \"you'd\",\n",
              " 'your',\n",
              " 'yours',\n",
              " 'yourself',\n",
              " 'yourselves',\n",
              " 'he',\n",
              " 'him',\n",
              " 'his',\n",
              " 'himself',\n",
              " 'she',\n",
              " \"she's\",\n",
              " 'her',\n",
              " 'hers',\n",
              " 'herself',\n",
              " 'it',\n",
              " \"it's\",\n",
              " 'its',\n",
              " 'itself',\n",
              " 'they',\n",
              " 'them',\n",
              " 'their',\n",
              " 'theirs',\n",
              " 'themselves',\n",
              " 'what',\n",
              " 'which',\n",
              " 'who',\n",
              " 'whom',\n",
              " 'this',\n",
              " 'that',\n",
              " \"that'll\",\n",
              " 'these',\n",
              " 'those',\n",
              " 'am',\n",
              " 'is',\n",
              " 'are',\n",
              " 'was',\n",
              " 'were',\n",
              " 'be',\n",
              " 'been',\n",
              " 'being',\n",
              " 'have',\n",
              " 'has',\n",
              " 'had',\n",
              " 'having',\n",
              " 'do',\n",
              " 'does',\n",
              " 'did',\n",
              " 'doing',\n",
              " 'a',\n",
              " 'an',\n",
              " 'the',\n",
              " 'and',\n",
              " 'but',\n",
              " 'if',\n",
              " 'or',\n",
              " 'because',\n",
              " 'as',\n",
              " 'until',\n",
              " 'while',\n",
              " 'of',\n",
              " 'at',\n",
              " 'by',\n",
              " 'for',\n",
              " 'with',\n",
              " 'about',\n",
              " 'against',\n",
              " 'between',\n",
              " 'into',\n",
              " 'through',\n",
              " 'during',\n",
              " 'before',\n",
              " 'after',\n",
              " 'above',\n",
              " 'below',\n",
              " 'to',\n",
              " 'from',\n",
              " 'up',\n",
              " 'down',\n",
              " 'in',\n",
              " 'out',\n",
              " 'on',\n",
              " 'off',\n",
              " 'over',\n",
              " 'under',\n",
              " 'again',\n",
              " 'further',\n",
              " 'then',\n",
              " 'once',\n",
              " 'here',\n",
              " 'there',\n",
              " 'when',\n",
              " 'where',\n",
              " 'why',\n",
              " 'how',\n",
              " 'all',\n",
              " 'any',\n",
              " 'both',\n",
              " 'each',\n",
              " 'few',\n",
              " 'more',\n",
              " 'most',\n",
              " 'other',\n",
              " 'some',\n",
              " 'such',\n",
              " 'no',\n",
              " 'nor',\n",
              " 'not',\n",
              " 'only',\n",
              " 'own',\n",
              " 'same',\n",
              " 'so',\n",
              " 'than',\n",
              " 'too',\n",
              " 'very',\n",
              " 's',\n",
              " 't',\n",
              " 'can',\n",
              " 'will',\n",
              " 'just',\n",
              " 'don',\n",
              " \"don't\",\n",
              " 'should',\n",
              " \"should've\",\n",
              " 'now',\n",
              " 'd',\n",
              " 'll',\n",
              " 'm',\n",
              " 'o',\n",
              " 're',\n",
              " 've',\n",
              " 'y',\n",
              " 'ain',\n",
              " 'aren',\n",
              " \"aren't\",\n",
              " 'couldn',\n",
              " \"couldn't\",\n",
              " 'didn',\n",
              " \"didn't\",\n",
              " 'doesn',\n",
              " \"doesn't\",\n",
              " 'hadn',\n",
              " \"hadn't\",\n",
              " 'hasn',\n",
              " \"hasn't\",\n",
              " 'haven',\n",
              " \"haven't\",\n",
              " 'isn',\n",
              " \"isn't\",\n",
              " 'ma',\n",
              " 'mightn',\n",
              " \"mightn't\",\n",
              " 'mustn',\n",
              " \"mustn't\",\n",
              " 'needn',\n",
              " \"needn't\",\n",
              " 'shan',\n",
              " \"shan't\",\n",
              " 'shouldn',\n",
              " \"shouldn't\",\n",
              " 'wasn',\n",
              " \"wasn't\",\n",
              " 'weren',\n",
              " \"weren't\",\n",
              " 'won',\n",
              " \"won't\",\n",
              " 'wouldn',\n",
              " \"wouldn't\"]"
            ]
          },
          "metadata": {},
          "execution_count": 73
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import PorterStemmer"
      ],
      "metadata": {
        "id": "dQhg7tZyjl00"
      },
      "execution_count": 74,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "stemer = PorterStemmer()"
      ],
      "metadata": {
        "id": "7dJ2AZknkIgo"
      },
      "execution_count": 75,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sentences = nltk.sent_tokenize(paragraph)"
      ],
      "metadata": {
        "id": "p5kMr1QXkMgM"
      },
      "execution_count": 77,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "type(sentences)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v9ftHVUzkXhA",
        "outputId": "374bd208-f701-49a7-88c9-6bf8a420dba3"
      },
      "execution_count": 78,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "list"
            ]
          },
          "metadata": {},
          "execution_count": 78
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "### Apply stopwords and Filter and Apply Stemming"
      ],
      "metadata": {
        "id": "HKomzRAzkc2f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(len(sentences)):\n",
        "  words = nltk.word_tokenize(sentences[i])\n",
        "  words = [stemer.stem(word) for word in words if word not in set(stopwords.words('english'))]\n",
        "  sentences[i] = ' '.join(words)## converting all the list of words into sentences"
      ],
      "metadata": {
        "id": "TbHuTFHSy1YN"
      },
      "execution_count": 82,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for sentence in sentences:\n",
        "  print(sentence)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "41JRq9u_zNwl",
        "outputId": "dc6a87f8-13f4-4052-ca5c-632fe256598c"
      },
      "execution_count": 83,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "grew eastern cape , area south africa much like one well describ thoma hardi .\n",
            "world oral tradit , heal properti herbal , abscess would treat poultic , clean water simpli avail .\n",
            "gastro intestin infect , malaria , cholera rampant .\n",
            "life brutish short .\n",
            "electr horseless carriag exist .\n",
            "hard poverti exist aggrav uncar societi .\n",
            "coloni meant exploit mother countri came settl area .\n",
            "fresh memori older gener poorest poor , spoken low hush tone , said huge profit busi said abolish – busi slaveri .\n",
            "youth young adulthood spent fight unjust oppress system .\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import SnowballStemmer\n",
        "snowball_stemmer = SnowballStemmer('english')"
      ],
      "metadata": {
        "id": "n3aB9jlHzWBn"
      },
      "execution_count": 87,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(len(sentences)):\n",
        "  words = nltk.word_tokenize(sentences[i])\n",
        "  words = [snowball_stemmer.stem(word) for word in words if word not in set(stopwords.words('english'))]\n",
        "  sentences[i] = ' '.join(words)## converting all the list of words into sentences"
      ],
      "metadata": {
        "id": "nKspzEsQ0MUf"
      },
      "execution_count": 88,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for sentence in sentences:\n",
        "  print(sentence)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iAVTQ26E0Ry8",
        "outputId": "0f1d22cf-9f73-4307-e41c-9581c85f989d"
      },
      "execution_count": 89,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "grew eastern cape , area south africa much like one well describ thoma hardi .\n",
            "world oral tradit , heal properti herbal , abscess would treat poultic , clean water simpli avail .\n",
            "gastro intestin infect , malaria , cholera rampant .\n",
            "life brutish short .\n",
            "electr horseless carriag exist .\n",
            "hard poverti exist aggrav uncar societi .\n",
            "coloni meant exploit mother countri came settl area .\n",
            "fresh memori older gener poorest poor , spoken low hush tone , said huge profit busi said abolish – busi slaveri .\n",
            "youth young adulthood spent fight unjust oppress system .\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import WordNetLemmatizer\n",
        "lemmatizer = WordNetLemmatizer()"
      ],
      "metadata": {
        "id": "f-TAbflL0T32"
      },
      "execution_count": 91,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(len(sentences)):\n",
        "  words = nltk.word_tokenize(sentences[i])\n",
        "  words = [lemmatizer.lemmatize(word, pos='v') for word in words if word not in set(stopwords.words('english'))]\n",
        "  sentences[i] = ' '.join(words)"
      ],
      "metadata": {
        "id": "J1YxmTIA0i_6"
      },
      "execution_count": 97,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for sentence in sentences:\n",
        "  print(sentence)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0g0FN8ee0mrO",
        "outputId": "3aa56e84-6e59-4754-993c-ab7e55f1ffeb"
      },
      "execution_count": 98,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "grow eastern cape , area south africa much like one well describ thoma hardi .\n",
            "world oral tradit , heal properti herbal , abscess would treat poultic , clean water simpli avail .\n",
            "gastro intestin infect , malaria , cholera rampant .\n",
            "life brutish short .\n",
            "electr horseless carriag exist .\n",
            "hard poverti exist aggrav uncar societi .\n",
            "coloni mean exploit mother countri come settl area .\n",
            "fresh memori older gener poorest poor , speak low hush tone , say huge profit busi say abolish – busi slaveri .\n",
            "youth young adulthood spend fight unjust oppress system .\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Parts of speech tagging"
      ],
      "metadata": {
        "id": "_Y_7kOKs0vm-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"Taj Mahal is a beautiful monument\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "2QX4_dWp2qQX",
        "outputId": "3a0967f4-3851-4e11-b515-1515216bd23a"
      },
      "execution_count": 100,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Taj Mahal is a beautiful monument'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 100
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sentences = nltk.sent_tokenize(paragraph)"
      ],
      "metadata": {
        "id": "uxdiMt5Z2x0H"
      },
      "execution_count": 101,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('averaged_perceptron_tagger_eng')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gt7BS6Kf5X9f",
        "outputId": "0364b217-3592-4ce2-9dd9-87afb6e9bcc8"
      },
      "execution_count": 109,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger_eng.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 109
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tkinter.constants import W\n",
        "## We will find out the Pos Tag\n",
        "for i in range(len(sentences)):\n",
        "  words = nltk.word_tokenize(sentences[i])\n",
        "  words = [word for word in words if word not in set(stopwords.words('english'))]\n",
        "  # sentences[i] = ' '.join(words)\n",
        "  pos_tag = nltk.pos_tag(words)\n",
        "  print(pos_tag)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7r6FZBs73ie5",
        "outputId": "0b3bb49c-d2b3-4b68-c484-095d318bd8a7"
      },
      "execution_count": 110,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('I', 'PRP'), ('grew', 'VBD'), ('Eastern', 'NNP'), ('Cape', 'NNP'), (',', ','), ('area', 'NN'), ('South', 'NNP'), ('Africa', 'NNP'), ('much', 'RB'), ('like', 'IN'), ('one', 'CD'), ('well', 'RB'), ('described', 'VBN'), ('Thomas', 'NNP'), ('Hardy', 'NNP'), ('.', '.')]\n",
            "[('It', 'PRP'), ('world', 'NN'), ('oral', 'JJ'), ('tradition', 'NN'), (',', ','), ('healing', 'VBG'), ('properties', 'NNS'), ('herbal', 'VBP'), (',', ','), ('abscess', 'RB'), ('would', 'MD'), ('treated', 'VB'), ('poultices', 'NNS'), (',', ','), ('clean', 'JJ'), ('water', 'NN'), ('simply', 'RB'), ('available', 'JJ'), ('.', '.')]\n",
            "[('Gastro', 'NNP'), ('intestinal', 'JJ'), ('infections', 'NNS'), (',', ','), ('malaria', 'NNS'), (',', ','), ('cholera', 'NN'), ('rampant', 'NN'), ('.', '.')]\n",
            "[('Life', 'NNP'), ('brutish', 'JJ'), ('short', 'JJ'), ('.', '.')]\n",
            "[('Electricity', 'NNP'), ('horseless', 'NN'), ('carriage', 'NN'), ('exist', 'NN'), ('.', '.')]\n",
            "[('The', 'DT'), ('hardness', 'NN'), ('poverty', 'NN'), ('existence', 'NN'), ('aggravated', 'VBD'), ('uncaring', 'JJ'), ('society', 'NN'), ('.', '.')]\n",
            "[('Colonies', 'NNS'), ('meant', 'VBP'), ('exploited', 'VBN'), ('mother', 'JJ'), ('country', 'NN'), ('came', 'VBD'), ('settle', 'JJ'), ('area', 'NN'), ('.', '.')]\n",
            "[('Fresh', 'JJ'), ('memories', 'NNS'), ('older', 'VBP'), ('generation', 'NN'), ('poorest', 'JJS'), ('poor', 'JJ'), (',', ','), ('spoken', 'JJ'), ('low', 'JJ'), ('hushed', 'VBN'), ('tones', 'NNS'), (',', ','), ('said', 'VBD'), ('hugely', 'RB'), ('profitable', 'JJ'), ('business', 'NN'), ('said', 'VBD'), ('abolished', 'VBN'), ('–', 'NNP'), ('business', 'NN'), ('slavery', 'NN'), ('.', '.')]\n",
            "[('My', 'PRP$'), ('youth', 'NN'), ('young', 'JJ'), ('adulthood', 'NN'), ('spent', 'VBN'), ('others', 'NNS'), ('fighting', 'VBG'), ('unjust', 'JJ'), ('oppressive', 'JJ'), ('system', 'NN'), ('.', '.')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "words = nltk.word_tokenize(\"Mt Everest is the highest peak in the world\")\n",
        "pos_tag = nltk.pos_tag(words)\n",
        "print(pos_tag)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r5Wsbrqo4TO3",
        "outputId": "bf1f484f-a9a9-4b10-c1f9-6fcaf289420b"
      },
      "execution_count": 111,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('Mt', 'NNP'), ('Everest', 'NNP'), ('is', 'VBZ'), ('the', 'DT'), ('highest', 'JJS'), ('peak', 'NN'), ('in', 'IN'), ('the', 'DT'), ('world', 'NN')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Name Entity Recognition"
      ],
      "metadata": {
        "id": "NIGiabNJ6PpG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sentence =\"Krishna Mandir is a 17th-century Shikhara-style temple built by King Siddhi Narsing Malla.It is located at the Patan Durbar Square, a UNESCO World Heritage Site, in the city of Lalitpur in Nepal.[2] It was damaged by the Nepal earthquake of April 2015, and was later restored in 2018.\"\n"
      ],
      "metadata": {
        "id": "gI7hgjtl6pG-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "words = nltk.word_tokenize(sentence)"
      ],
      "metadata": {
        "id": "3H2Ek_nH7WqL"
      },
      "execution_count": 113,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('words')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mht2Fp6V8Ifg",
        "outputId": "6b2dcd02-a9bf-4ceb-9091-604aba2e863f"
      },
      "execution_count": 118,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package words to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/words.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 118
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tagged_words = nltk.pos_tag(words)"
      ],
      "metadata": {
        "id": "n4sXoAJD7aCu"
      },
      "execution_count": 114,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('maxent_ne_chunker_tab')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5_uRVqc97vgK",
        "outputId": "a390418e-6c9e-454d-cca5-bafdb52d2f0f"
      },
      "execution_count": 116,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package maxent_ne_chunker_tab to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping chunkers/maxent_ne_chunker_tab.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 116
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install svgling"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "43wFQd5I8bnS",
        "outputId": "5b0befed-765d-41e4-9f6b-439e7a968f24"
      },
      "execution_count": 120,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting svgling\n",
            "  Downloading svgling-0.5.0-py3-none-any.whl.metadata (7.4 kB)\n",
            "Collecting svgwrite (from svgling)\n",
            "  Downloading svgwrite-1.4.3-py3-none-any.whl.metadata (8.8 kB)\n",
            "Downloading svgling-0.5.0-py3-none-any.whl (31 kB)\n",
            "Downloading svgwrite-1.4.3-py3-none-any.whl (67 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.1/67.1 kB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: svgwrite, svgling\n",
            "Successfully installed svgling-0.5.0 svgwrite-1.4.3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.ne_chunk(tagged_words)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 142
        },
        "id": "n7ZXWYFA7ieo",
        "outputId": "e5924116-e591-4f6e-9335-85ac5da8fc67"
      },
      "execution_count": 124,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Tree('S', [('youth', 'NN'), ('young', 'JJ'), ('adulthood', 'NN'), ('spend', 'VB'), ('fight', 'JJ'), ('unjust', 'NN'), ('oppress', 'NN'), ('system', 'NN'), ('.', '.')])"
            ],
            "image/svg+xml": "<svg baseProfile=\"full\" height=\"120px\" preserveAspectRatio=\"xMidYMid meet\" style=\"font-family: times, serif; font-weight: normal; font-style: normal; font-size: 16px\" version=\"1.1\" viewBox=\"0,0,536.0,120.0\" width=\"536px\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:ev=\"http://www.w3.org/2001/xml-events\" xmlns:xlink=\"http://www.w3.org/1999/xlink\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">S</text></svg><svg width=\"10.4478%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">youth</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">NN</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"20px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"5.22388%\" y1=\"20px\" y2=\"48px\" /><svg width=\"10.4478%\" x=\"10.4478%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">young</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">JJ</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"20px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"15.6716%\" y1=\"20px\" y2=\"48px\" /><svg width=\"16.4179%\" x=\"20.8955%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">adulthood</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">NN</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"20px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"29.1045%\" y1=\"20px\" y2=\"48px\" /><svg width=\"10.4478%\" x=\"37.3134%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">spend</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">VB</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"20px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"42.5373%\" y1=\"20px\" y2=\"48px\" /><svg width=\"10.4478%\" x=\"47.7612%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">fight</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">JJ</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"20px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"52.9851%\" y1=\"20px\" y2=\"48px\" /><svg width=\"11.9403%\" x=\"58.209%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">unjust</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">NN</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"20px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"64.1791%\" y1=\"20px\" y2=\"48px\" /><svg width=\"13.4328%\" x=\"70.1493%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">oppress</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">NN</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"20px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"76.8657%\" y1=\"20px\" y2=\"48px\" /><svg width=\"11.9403%\" x=\"83.5821%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">system</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">NN</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"20px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"89.5522%\" y1=\"20px\" y2=\"48px\" /><svg width=\"4.47761%\" x=\"95.5224%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">.</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">.</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"20px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"97.7612%\" y1=\"20px\" y2=\"48px\" /></svg>"
          },
          "metadata": {},
          "execution_count": 124
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "3IxkJ9gC7oaW"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}